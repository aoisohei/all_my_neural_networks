{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Functions_for_peceptron.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRjQ-7MLb_Qb"
      },
      "source": [
        "# My data to function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S4k6sJDkFai",
        "outputId": "822758b5-2ce9-4a89-b129-2e804db66f96"
      },
      "source": [
        " 0.475 + 0.545 + 0.61"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.63"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTTJh0yhahx8"
      },
      "source": [
        "# Softmax Function\r\n",
        "\r\n",
        "![](https://i.imgur.com/tYoHtiW.png)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U17pljIonQ-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c71995-dde6-4667-84ef-277083727630"
      },
      "source": [
        "# transform values into probabilities\r\n",
        "from math import exp\r\n",
        "\r\n",
        "# calculate each probability\r\n",
        "p1 = exp(0.475) / (exp(0.475) + exp(0.545) + exp(0.61))\r\n",
        "p2 = exp(0.545) / (exp(0.475) + exp(0.545) + exp(0.61))\r\n",
        "p3 = exp(0.61) / (exp(0.475) + exp(0.545) + exp(0.61))\r\n",
        "\r\n",
        "# report probabilities\r\n",
        "print(p1, p2, p3)\r\n",
        "\r\n",
        "# report sum of probabilities\r\n",
        "print(p1 + p2 + p3)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3108442718990016 0.3333830247076894 0.35577270339330896\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55-FKInjbwHE"
      },
      "source": [
        "Softmax Function with Python, Types of other functions : \r\n",
        "- https://machinelearningmastery.com/softmax-activation-function-with-python/\r\n",
        "\r\n",
        "Wiki, explication of Softmax Function : \r\n",
        "- https://www.google.com/search?q=function+activation+softmax&oq=function+activation+softmax&aqs=chrome..69i57.6793j0j7&sourceid=chrome&ie=UTF-8\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NauLyh_IggAj"
      },
      "source": [
        "## Hyperbolic Tangent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqLSX3nwgnZI",
        "outputId": "c9ff5a1f-e019-4bc6-eab9-650049658a8d"
      },
      "source": [
        "x = 1.63\r\n",
        "tanh_x_1 = exp(x) - exp(-x) \r\n",
        "tanh_x_2 = exp(x) + exp(-x) \r\n",
        "tanh_x = tanh_x_1 / tanh_x_2\r\n",
        "tanh_x"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.926061581406646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TuLgMdniZjV"
      },
      "source": [
        "tanh(x) = 0.92\r\n",
        "\r\n",
        "randian = 0.75\r\n",
        "\r\n",
        "***Hyperbolic Tangent Function***\r\n",
        "\r\n",
        "https://sefiks.com/2017/01/29/hyperbolic-tangent-as-neural-network-activation-function/#:~:text=In%20neural%20networks%2C%20as%20an,for%20error%20effects%20on%20weights.&text=In%20other%20words%2C%20function%20produces%20output%20for%20every%20x%20value ."
      ]
    }
  ]
}
